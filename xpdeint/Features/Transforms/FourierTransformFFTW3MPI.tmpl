@*
FourierTransformFFTW3MPI.tmpl

Created by Graham Dennis on 2008-06-06.
Copyright (c) 2008 __MyCompanyName__. All rights reserved.
*@
@extends xpdeint.Features.Transforms._FourierTransformFFTW3MPI
@import operator
@from xpdeint.Geometry.UniformDimensionRepresentation import UniformDimensionRepresentation
@from xpdeint.Geometry.SplitUniformDimensionRepresentation import SplitUniformDimensionRepresentation

@def description: FFTW3 with MPI
@attr $fftwSuffix = 'mpi'

@def includes
  @#
  @super
  @#
#include <fftw3-mpi.h>
@end def

@def globals
  @#
  @super
  @#
ptrdiff_t _unswapped_block_size = 0;
ptrdiff_t _swapped_block_size = 0;
  @#
@end def

@def transformTypes
  @#
  @super
  @#
, _MPI_DFT_TRANSFORM
, _MPI_R2R_TRANSFORM
, _MPI_TRANSPOSE
  @#
@end def

@def mainBegin($dict)
  @#
ptrdiff_t _fft_sizes[${len($geometry.dimensions)}];
  @super($dict)
  @#
@end def


@def setLocalLatticeAndOffsetVariables
  @#
// First work out the local lattice and offset for the geometry
  @set $firstMPIDim = $mpiDimensions[0]
  @set $secondMPIDim = $mpiDimensions[1]
  @set $fullTransformDimensions = $fullTransformDimensionsForField($geometry)
ptrdiff_t _sizes[] = {${', '.join([dim.inSpace(0).globalLattice for dim in fullTransformDimensions])}};

  @set $indices = range($geometry.indexOfDimension(fullTransformDimensions[-1])+1, len($geometry.dimensions))
ptrdiff_t _howmany = ((ptrdiff_t)1) * ${geometry.pointsInDimensionsWithIndices(indices)};
ptrdiff_t _local_alloc_size;

${fftwPrefix}_mpi_init();
_local_alloc_size = ${fftwPrefix}_mpi_local_size_many_transposed(${len(fullTransformDimensions)}, _sizes, _howmany,
                                                        FFTW_MPI_DEFAULT_BLOCK, FFTW_MPI_DEFAULT_BLOCK, MPI_COMM_WORLD,
                                                        &${firstMPIDim.inSpace(0).localLattice}, &${firstMPIDim.inSpace(0).localOffset},
                                                        &${secondMPIDim.inSpace($swappedSpace).localLattice}, &${secondMPIDim.inSpace($swappedSpace).localOffset});
  @# Set the transformed, but unswapped variables for the first dimension
${firstMPIDim.inSpace($firstMPIDim.transformMask).localLattice} = ${firstMPIDim.inSpace(0).localLattice};
${firstMPIDim.inSpace($firstMPIDim.transformMask).localOffset} = ${firstMPIDim.inSpace(0).localOffset};

if (_rank == 0) {
  _unswapped_block_size = ${firstMPIDim.inSpace(0).localLattice};
  _swapped_block_size = ${secondMPIDim.inSpace($swappedSpace).localLattice};
}
MPI_Bcast(&_unswapped_block_size, sizeof(ptrdiff_t), MPI_BYTE, 0, MPI_COMM_WORLD);
MPI_Bcast(&_swapped_block_size, sizeof(ptrdiff_t), MPI_BYTE, 0, MPI_COMM_WORLD);

  @for $field in $fields
    @if field.name == 'geometry' or not $field.isDistributed
      @continue
    @end if
    @#
    @# Set the local_lattice and local_offset variables based on the
    @# values for the geometry's version of these
    @set $fieldMPIDim1 = field.dimensionWithName(firstMPIDim.name)
    @set $fieldMPIDim2 = field.dimensionWithName(secondMPIDim.name)
// Set the local lattice and offset variables for the '${field.name}' field
    @if fieldMPIDim1 == firstMPIDim and fieldMPIDim2 == secondMPIDim
      @# The field has the same structure for these dimensions as the geometry
      @for dimOut, dimIn in [(fieldMPIDim1, firstMPIDim), (fieldMPIDim2, secondMPIDim)]
        @for repOut, repIn in zip(dimOut.representations, dimIn.representations)
          @if repOut and repOut.hasLocalOffset and repOut.parent is fieldMPIDim1
${repOut.localLattice} = ${repIn.localLattice};
${repOut.localOffset} = ${repIn.localOffset};
          @end if
        @end for
      @end for
      @if all([dim.isTransformable for dim in [fieldMPIDim1, fieldMPIDim2]])
        @# Set the alloc size variable through an actual call to local_size_many_transposed
        @set $indices = range(len(field.dimensions))
        @for dim in [fieldMPIDim1, fieldMPIDim2]
          @silent indices.remove(field.indexOfDimension(dim))
        @end for
_howmany = ((ptrdiff_t)1) * ${field.pointsInDimensionsWithIndices(indices)};
${field.allocSize} = ${fftwPrefix}_mpi_local_size_many_transposed(2, _sizes, _howmany, _unswapped_block_size, _swapped_block_size, MPI_COMM_WORLD,
        &${fieldMPIDim1.inSpace(0).localLattice}, &${fieldMPIDim1.inSpace(0).localOffset},
        &${fieldMPIDim2.inSpace($swappedSpace).localLattice}, &${fieldMPIDim2.inSpace($swappedSpace).localOffset});
        @if any([dimRep.parent is mpiDim for mpiDim in [fieldMPIDim1, fieldMPIDim2] for dimRep in mpiDim.representations])
if (   (${fieldMPIDim1.inSpace(0).localLattice} != ${firstMPIDim.inSpace(0).localLattice})
    || (${fieldMPIDim1.inSpace(0).localOffset} != ${firstMPIDim.inSpace(0).localOffset})
    || (${fieldMPIDim2.inSpace($swappedSpace).localLattice} != ${secondMPIDim.inSpace($swappedSpace).localLattice})
    || (${fieldMPIDim2.inSpace($swappedSpace).localOffset} != ${secondMPIDim.inSpace($swappedSpace).localOffset})) {
  _LOG(_ERROR_LOG_LEVEL, "Local lattice/offset values returned for field '$field.name' aren't the same as the values for the full transform.\n"
                         "Please report this error and the script that caused it to ${bugReportAddress}\n"
                         "Number of nodes: %i\n"
                         "${firstMPIDim.inSpace(0).localLattice}: %ti\n"
                         "${firstMPIDim.inSpace(0).localOffset}: %ti\n"
                         "${fieldMPIDim1.inSpace(0).localLattice}: %ti\n"
                         "${fieldMPIDim1.inSpace(0).localOffset}: %ti\n"
                         "${secondMPIDim.inSpace($swappedSpace).localLattice}: %ti\n"
                         "${secondMPIDim.inSpace($swappedSpace).localOffset}: %ti\n"
                         "${fieldMPIDim2.inSpace($swappedSpace).localLattice}: %ti\n"
                         "${fieldMPIDim2.inSpace($swappedSpace).localOffset}: %ti\n",
                         _size, ${firstMPIDim.inSpace(0).localLattice}, ${firstMPIDim.inSpace(0).localOffset},
                         ${fieldMPIDim1.inSpace(0).localLattice}, ${fieldMPIDim1.inSpace(0).localOffset},
                         ${secondMPIDim.inSpace($swappedSpace).localLattice}, ${secondMPIDim.inSpace($swappedSpace).localOffset},
                         ${fieldMPIDim2.inSpace($swappedSpace).localLattice}, ${fieldMPIDim2.inSpace($swappedSpace).localOffset});
}
        @end if
      @else
              @set $fieldIndices = range(0, len(field.dimensions))
              @silent fieldIndices.remove(field.indexOfDimension(firstMPIDim))
              @silent fieldIndices.remove(field.indexOfDimension(secondMPIDim))
              @set $geometryIndices = range(1, len($geometry.dimensions))
              @silent geometryIndices.remove($geometry.indexOfDimension(firstMPIDim))
              @silent geometryIndices.remove($geometry.indexOfDimension(secondMPIDim))
${field.allocSize} = _local_alloc_size * ${field.pointsInDimensionsWithIndices(fieldIndices)} / (${geometry.pointsInDimensionsWithIndices(geometryIndices)});
      @end if
    @else
      @# The field has different structure to the geometry, so we need to work out our local offsets and lattice sizes.
      @for fieldDim, mpiDim in [(fieldMPIDim1, firstMPIDim), (fieldMPIDim2, secondMPIDim)]
        @if fieldDim == mpiDim
          @# This dimension matches.
          @for repOut, repIn in zip(fieldDim.representations, mpiDim.representations)
            @if repOut and repOut.hasLocalOffset
${repOut.localLattice} = ${repIn.localLattice};
${repOut.localOffset} = ${repIn.localOffset};
            @end if
          @end for
        @else
          @# The dimensions aren't the same.
          @for fieldRep, geometryRep in zip(fieldDim.representations, mpiDim.representations)
            @if not fieldRep or not fieldRep.hasLocalOffset
              @continue
            @end if
            @if fieldRep.reductionMethod == fieldDim.ReductionMethod.fixedRange
              @# In this case we are in 'x' space and are subdividing a distributed dimension
ptrdiff_t _${field.name}_unswapped_skip_size = ${geometryRep.globalLattice}/${fieldRep.globalLattice};
if (_rank == 0) {
  ${fieldRep.localOffset}  = 0;
  ${fieldRep.localLattice} = (${geometryRep.localLattice}-1)/_${field.name}_unswapped_skip_size + 1;
} else {
  ${fieldRep.localOffset}  = (${geometryRep.localOffset}-1)/_${field.name}_unswapped_skip_size + 1;
  ${fieldRep.localLattice} = (${geometryRep.localOffset} + ${geometryRep.localLattice} - 1)/_${field.name}_unswapped_skip_size
                             + 1 - ${fieldRep.localOffset};
}
            @elif isinstance(fieldRep, UniformDimensionRepresentation)
              @# In this case, we are in 'k' space and may be subdividing a UniformDimensionRepresentation (dct/dst)
if (${geometryRep.localOffset} >= ${fieldRep.globalLattice}) {
  // No points here
  ${fieldRep.localOffset} = 0;
  ${fieldRep.localLattice} = 0;
} else if (${geometryRep.localOffset} + ${geometryRep.localLattice} > ${fieldRep.globalLattice}){
  // The upper edge is here
  ${fieldRep.localOffset} = ${geometryRep.localOffset};
  ${fieldRep.localLattice} = ${fieldRep.globalLattice} - ${geometryRep.localOffset};
} else {
  // somewhere near the start
  ${fieldRep.localOffset} = ${geometryRep.localOffset};
  ${fieldRep.localLattice} = ${geometryRep.localLattice};
}
            @elif isinstance(fieldRep, SplitUniformDimensionRepresentation)
              @# In this case, we are in 'k' space and may be subdividing a SplitUniformDimensionRepresentation (dft)
${fieldRep.localOffset} = -1;
if (${geometryRep.localOffset} >= (${fieldRep.globalLattice}+1)/2) {
  // No points due to positive 'k' values.
} else if (${geometryRep.localOffset} + ${geometryRep.localLattice} > (${fieldRep.globalLattice}+1)/2) {
  // the upper edge of the positive values are here
  ${fieldRep.localOffset} = ${geometryRep.localOffset};
  ${fieldRep.localLattice} = (${fieldRep.globalLattice}+1)/2 - ${geometryRep.localOffset};
} else if (${geometryRep.localOffset} < (${fieldRep.globalLattice}+1)/2) {
  // somewhere near the start of the positive values
  ${fieldRep.localOffset} = ${geometryRep.localOffset};
  ${fieldRep.localLattice} = ${geometryRep.localLattice};
}

if (${geometryRep.localOffset} + ${geometryRep.localLattice} <= ${geometryRep.globalLattice} - ${fieldRep.globalLattice}/2) {
  // No points due to negative 'k' values.
} else if (${geometryRep.localOffset} < ${geometryRep.globalLattice} - ${fieldRep.globalLattice}/2) {
  // the lower edge of the negative values are here
  if (${fieldRep.localOffset} == -1)
    ${fieldRep.localOffset} = (${fieldRep.globalLattice}+1)/2;
  ${fieldRep.localLattice} += ${geometryRep.localLattice} - (${geometryRep.globalLattice}-${fieldRep.globalLattice}/2-${geometryRep.localOffset});
} else if (${geometryRep.localOffset} + ${geometryRep.localLattice} > ${geometryRep.globalLattice} - ${fieldRep.globalLattice}/2) {
  // somewhere near the end of the negative values
  ${fieldRep.localOffset} = ${geometryRep.localOffset} - (${geometryRep.globalLattice}-${fieldRep.globalLattice});
  ${fieldRep.localLattice} = ${geometryRep.localLattice};
}
            @end if
          @end for
        @end if
      @end for
ptrdiff_t _${field.name}_2d_size = 0;
      @for spaceID in range(4)
        @set $space = 0
        @if spaceID & 1
          @set $space |= $firstMPIDim.transformMask
        @end if
        @if spaceID & 2
          @set $space |= $secondMPIDim.transformMask
        @end if
        @set $fieldRep1 = $fieldMPIDim1.inSpace(space)
        @set $fieldRep2 = $fieldMPIDim2.inSpace(space)
        @if not fieldRep1 or not fieldRep2
          @continue
        @end if
_${field.name}_2d_size = MAX(_${field.name}_2d_size, ${fieldRep1.localLattice} * ${fieldRep2.localLattice});
      @end for
      @set $indices = range(len(field.dimensions))
      @silent indices.remove(field.indexOfDimension(firstMPIDim))
      @silent indices.remove(field.indexOfDimension(secondMPIDim))
${field.allocSize} = _${field.name}_2d_size * ${field.pointsInDimensionsWithIndices(indices)};
    @end if
  @end for
  @#
@end def

@def transposeTransformFunction(transformID, transformDict, function)
  @#
  @set mpiPrefix, prefixLattice, postfixLattice = transformDict['transformSpecifier']
// _prefix_lattice should be ${prefixLattice}
// _postfix_lattice should be ${postfixLattice}
static ${fftwPrefix}_plan _fftw_forward_plan = NULL;
static ${fftwPrefix}_plan _fftw_backward_plan = NULL;

if (!_fftw_forward_plan) {
  _LOG(_SIMULATION_LOG_LEVEL, "Planning for ${function.description}...");
  @set $transformPair = transformDict['transformPair']
  @set $dataOut = '_data_out' if transformDict.get('outOfPlace', False) else '_data_in'
  
  _fftw_forward_plan = ${fftwPrefix}_mpi_plan_many_transpose(
    ${', '.join(dr.globalLattice for dr in transformPair[0])},
    _postfix_lattice, _unswapped_block_size, _swapped_block_size,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>($dataOut),
    MPI_COMM_WORLD, ${planType}
  );
  
  if (!_fftw_forward_plan)
    _LOG(_ERROR_LOG_LEVEL, "(%s: %i) Unable to create forward mpi transform plan.\n", __FILE__, __LINE__);
  
  _fftw_backward_plan = ${fftwPrefix}_mpi_plan_many_transpose(
    ${', '.join(dr.globalLattice for dr in transformPair[1])},
    _postfix_lattice, _swapped_block_size, _unswapped_block_size,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>($dataOut),
    MPI_COMM_WORLD, ${planType}
  );
  
  if (!_fftw_backward_plan)
    _LOG(_ERROR_LOG_LEVEL, "(%s: %i) Unable to create backward mpi transform plan.\n", __FILE__, __LINE__);
  
  // Save wisdom
  #if CFG_OSAPI == CFG_OSAPI_POSIX
  ${saveWisdom, autoIndent=True}@slurp
  #endif // POSIX
  
  _LOG(_SIMULATION_LOG_LEVEL, " done.\n");
}

if (_forward) {
  ${fftwPrefix}_execute_r2r(
    _fftw_forward_plan,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>(${dataOut})
  );
} else {
  ${fftwPrefix}_execute_r2r(
    _fftw_backward_plan,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>(${dataOut})
  );
}
  @#
@end def

@def distributedTransformFunction(transformID, transformDict, function)
  @#
  @set mpiPrefix, prefixLattice, postfixLattice = transformDict['transformSpecifier']
// _prefix_lattice should be ${prefixLattice}
// _postfix_lattice should be ${postfixLattice}
static ${fftwPrefix}_plan _fftw_forward_plan = NULL;
static ${fftwPrefix}_plan _fftw_backward_plan = NULL;

if (!_fftw_forward_plan) {
  _LOG(_SIMULATION_LOG_LEVEL, "Planning for ${function.description}...");
  @set $transformPair = transformDict['transformPair']
  @set $dimensionsBeingTransformed = len(transformPair[0])
  @set $transformType = transformDict['transformType']
  @set $dataOut = '_data_out' if transformDict.get('outOfPlace', False) else '_data_in'
  ptrdiff_t _transform_sizes[${dimensionsBeingTransformed}];
  @if transformType == 'real'
  ${fftwPrefix}_r2r_kind _r2r_kinds[${dimensionsBeingTransformed}];
  @end if
  
  int _transform_sizes_index = 0;
  
  @#
  @for dimID, dimRep in enumerate(transformPair[0])
  _transform_sizes[_transform_sizes_index++] = ${dimRep.globalLattice};
  @end for

  @if transformType == 'complex'
    @set $guruPlanFunction = self.createGuruMPIDFTPlanInDirection
  @else
    @set $guruPlanFunction = self.createGuruMPIR2RPlanInDirection
  @end if
  @#
  ${guruPlanFunction(
      transformDict, 'forward', dataOut,
      '_unswapped_block_size', '_swapped_block_size', 'FFTW_MPI_TRANSPOSED_OUT'
    ), autoIndent=True}@slurp
  ${guruPlanFunction(
      transformDict, 'backward', dataOut,
      '_swapped_block_size', '_unswapped_block_size', 'FFTW_MPI_TRANSPOSED_IN'
    ), autoIndent=True}@slurp
  
  // Save wisdom
  #if CFG_OSAPI == CFG_OSAPI_POSIX
  ${saveWisdom, autoIndent=True}@slurp
  #endif // POSIX
  
  _LOG(_SIMULATION_LOG_LEVEL, " done.\n");
}

if (_forward) {
  ${fftwPrefix}_execute_r2r(
    _fftw_forward_plan,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>(${dataOut})
  );
} else {
  ${fftwPrefix}_execute_r2r(
    _fftw_backward_plan,
    reinterpret_cast<real*>(_data_in),
    reinterpret_cast<real*>(${dataOut})
  );
}
  @#
@end def

@def createGuruMPIDFTPlanInDirection($transformDict, $direction, $dataOut, $inBlockSize, $outBlockSize, $transposedState)
  @#
_fftw_${direction}_plan = ${fftwPrefix}_mpi_plan_many_dft(
  _transform_sizes_index, _transform_sizes, _postfix_lattice,
  ${inBlockSize}, ${outBlockSize},
  reinterpret_cast<${fftwPrefix}_complex*>(_data_in),
  reinterpret_cast<${fftwPrefix}_complex*>(${dataOut}),
  MPI_COMM_WORLD, FFTW_${direction.upper()}, ${planType} | ${transposedState}
);
if (!_fftw_${direction}_plan)
  _LOG(_ERROR_LOG_LEVEL, "(%s: %i) Unable to create ${direction} mpi dft plan.\n", __FILE__, __LINE__);

  @#
@end def

@def createGuruMPIR2RPlanInDirection($transformDict, $direction, $dataOut, $inBlockSize, $outBlockSize, $transposedState)
  @#
  @for idx, dimRep in enumerate(transformDict['transformPair'][0])
_r2r_kinds[${idx}] = ${r2rKindForDimensionAndDirection(dimRep.name, direction)};
  @end for

_fftw_${direction}_plan = ${fftwPrefix}_mpi_plan_many_r2r(
  _transform_sizes_index, _transform_sizes, _postfix_lattice,
  ${inBlockSize}, ${outBlockSize},
  reinterpret_cast<real*>(_data_in),
  reinterpret_cast<real*>(${dataOut}),
  MPI_COMM_WORLD, _r2r_kinds, ${planType} | ${transposedState}
);

if (!_fftw_${direction}_plan)
  _LOG(_ERROR_LOG_LEVEL, "(%s: %i) Unable to create ${direction} mpi r2r plan.\n", __FILE__, __LINE__);
  @#
@end def


@def loadWisdom
  @#
  @super
  @#
${fftwPrefix}_mpi_broadcast_wisdom(MPI_COMM_WORLD);
  @#
@end def

@def saveWisdom
  @#
${fftwPrefix}_mpi_gather_wisdom(MPI_COMM_WORLD);
  @#
  @super
  @#
@end def


@def createMPIPlansForVector($vector, $direction, $transformDimensions, $space, $identifier)
  @#
  @set $r2rMultiplier = 1
  @if vector.type == 'complex' and not 'dft' in $distributedMPIKinds
    @set $r2rMultiplier = 2
  @end if
  @#
  @set $arrayName = c'_${vector.id}'
  @if vector in vector.field.temporaryVectors
    @set $arrayName = '_temporary_vector'
  @end if
  @#
_c = 1.0;
  @for i, dim in enumerate(transformDimensions)
_fft_sizes[$i] = ${dim.inSpace(space).globalLattice};
    @if 'dct' in $distributedMPIKinds
_r2r_kinds[$i]  = ${r2rKindForDimensionAndDirection(dim, direction)};
    @end if
_c *= ${dim.inSpace(space).stepSize} * _inverse_sqrt_2pi;
  @end for
  @#
  @set $rank = $i+1
_howmany = ${r2rMultiplier} * _${vector.id}_ncomponents;
  @for dim in vector.field.dimensions[rank:]
_howmany *= ${dim.inSpace(space).globalLattice};
  @end for

  @if direction == 'forward'
    @set $inBlockSize = '_unswapped_block_size'
    @set $outBlockSize = '_swapped_block_size'
    @set $transposedState = 'FFTW_MPI_TRANSPOSED_OUT'
  @else
    @set $inBlockSize = '_swapped_block_size'
    @set $outBlockSize = '_unswapped_block_size'
    @set $transposedState = 'FFTW_MPI_TRANSPOSED_IN'
  @end if
  @#
  @if 'dft' in $distributedMPIKinds
_plan = ${fftwPrefix}_mpi_plan_many_dft(${rank}, _fft_sizes, _howmany, ${inBlockSize}, ${outBlockSize}, 
                               reinterpret_cast<${fftwPrefix}_complex*>($arrayName),
                               reinterpret_cast<${fftwPrefix}_complex*>($arrayName),
                               MPI_COMM_WORLD, FFTW_${direction.upper}, ${planType} | ${transposedState});
  @else
_plan = ${fftwPrefix}_mpi_plan_many_r2r(${rank}, _fft_sizes, _howmany, ${inBlockSize}, ${outBlockSize},
                               reinterpret_cast<real*>($arrayName),
                               reinterpret_cast<real*>($arrayName),
                               MPI_COMM_WORLD, _r2r_kinds, ${planType} | ${transposedState});
  @end if
if (!_plan)
  _LOG(_ERROR_LOG_LEVEL, "(%s: %i) Unable to create $direction plan for vector ${vector.id}.\n", __FILE__, __LINE__);
  @if 'dft' in $distributedMPIKinds
    @set $transformType = '_MPI_DFT_TRANSFORM'
  @else
    @set $transformType = '_MPI_R2R_TRANSFORM'
  @end if
_space_transform _${vector.id}_${identifier}(_plan, $transformType, _c);
  @#
@end def

@def createGuruPlansForVector($vector)
  @#
  @if not $vector.field.isDistributed
    @super(vector)
    @stop
  @end if
  @#
  @set $r2rMultiplier = 1
  @if vector.type == 'complex'
    @set $r2rMultiplier = 2
  @end if
  @#
  @set $arrayName = c'_${vector.id}'
  @if vector in vector.field.temporaryVectors
    @set $arrayName = '_temporary_vector'
  @end if
  @#
  @set $transformDimensions = $fullTransformDimensionsForField(vector.field)
  @#
unsigned long _${vector.id}_full_mask = ${reduce(operator.__or__, [dim.transformMask for dim in transformDimensions])};
${createMPIPlansForVector(vector, 'forward', transformDimensions, space = 0, identifier = 'forward')}@slurp
${createMPIPlansForVector(vector, 'backward', transformDimensions, space = -1, identifier = 'backward')}@slurp

  @if $vectorNeedsPartialTransforms(vector)
    @# The idea here is to break the process of going from one space to another into two stages
    @# the first involving MPI, the second is purely local.
    @# In the first stage, the space of the first two dimensions are changed from their initial
    @# space to their values in the final space.
    @# In the second stage, local fourier transforms are performed on each node to perform the
    @# remaining transforms necessary.
    @# For more details on the transforms and transposes involved in the first stage, see
    @# "FFTW3MPI Design.pdf" in the doc/ directory of the repository.
// First create plans for changing the spaces of the first two dimensions arbitrarily
// then create (local) plans for the rest of the transforms

// 1a. Create plans for the 2D forward and reverse transforms.
    @set $transformDimensions = vector.field.dimensions[0:2]
${createMPIPlansForVector(vector, 'forward', transformDimensions, space = 0, identifier = 'partial_2d_forward')}@slurp
${createMPIPlansForVector(vector, 'backward', transformDimensions, space = $swappedSpace, identifier = 'partial_2d_backward')}@slurp

// 1b. Create plans for the 2D transpose operations.
    @set $transposeMultiplier = 2
    @if vector.type == 'real'
      @set $transposeMultiplier = 1
    @end if
_howmany = ${transposeMultiplier} * _${vector.id}_ncomponents;
    @for dim in vector.field.dimensions[2:]
_howmany *= ${dim.inSpace(0).globalLattice};
    @end for

_plan  = ${fftwPrefix}_mpi_plan_many_transpose(${mpiDimensions[0].inSpace(0).globalLattice},
                                      ${mpiDimensions[1].inSpace(0).globalLattice},
                                      _howmany, _unswapped_block_size, _swapped_block_size,
                                      reinterpret_cast<real*>($arrayName),
                                      reinterpret_cast<real*>($arrayName),
                                      MPI_COMM_WORLD, ${planType});
if (!_plan)
  _LOG(_ERROR_LOG_LEVEL, "Unable to create partial transpose plan for vector ${vector.id}\n");

_space_transform _${vector.id}_partial_transpose(_plan, _MPI_TRANSPOSE);

_plan = ${fftwPrefix}_mpi_plan_many_transpose(${mpiDimensions[1].inSpace(0).globalLattice},
                                     ${mpiDimensions[0].inSpace(0).globalLattice},
                                     _howmany, _swapped_block_size, _unswapped_block_size,
                                     reinterpret_cast<real*>($arrayName),
                                     reinterpret_cast<real*>($arrayName),
                                     MPI_COMM_WORLD, ${planType});
if (!_plan)
  _LOG(_ERROR_LOG_LEVEL, "Unable to create partial untranspose plan for vector ${vector.id}\n");

_space_transform _${vector.id}_partial_untranspose(_plan, _MPI_TRANSPOSE);

// 1c. Create local plans for unswapped fourier transforms
_forward_sizes_index = _backward_sizes_index = _r2r_sizes_index = 0;
_forward_loop_sizes_index = _backward_loop_sizes_index = _r2r_loop_sizes_index = 0;

//     Loop over the distributed dimension '$mpiDimensions[0].name'
    @if 'dft' in $distributedMPIKinds
_forward_iodim_ptr = &_forward_loop_sizes[_forward_loop_sizes_index++];
_backward_iodim_ptr = &_backward_loop_sizes[_backward_loop_sizes_index++];

_forward_iodim_ptr->n  = ${mpiDimensions[0].inSpace(0).localLattice};
_forward_iodim_ptr->is = _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(1, len(vector.field.dimensions)))};

_backward_iodim_ptr->n = _forward_iodim_ptr->n;
_backward_iodim_ptr->os = _forward_iodim_ptr->os = _backward_iodim_ptr->is = _forward_iodim_ptr->is;
    @else
_r2r_iodim_ptr = &_r2r_loop_sizes[_r2r_loop_sizes_index++];
_r2r_iodim_ptr->n = ${mpiDimensions[0].inSpace(0).localLattice};
_r2r_iodim_ptr->is = ${r2rMultiplier} * _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(1, len(vector.field.dimensions)))};
_r2r_iodim_ptr->os = _r2r_iodim_ptr->is;
    @end if

// Forward transform the local MPI dimension '$mpiDimensions[1].name'
    @if 'dft' in $distributedMPIKinds
_forward_iodim_ptr = &_forward_sizes[_forward_sizes_index++];
_backward_iodim_ptr = &_backward_sizes[_backward_sizes_index++];

_forward_iodim_ptr->n  = ${mpiDimensions[1].inSpace(0).localLattice};
_forward_iodim_ptr->is = _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};

_backward_iodim_ptr->n = _forward_iodim_ptr->n;
_backward_iodim_ptr->os = _forward_iodim_ptr->os = _backward_iodim_ptr->is = _forward_iodim_ptr->is;

_space_transform _${vector.id}_partial_unswapped_forward(${mpiDimensions[1].inSpace(0).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_unswapped_forward;
${createGuruDFTPlanForVector(vector, 'forward')}@slurp
_space_transform _${vector.id}_partial_unswapped_backward(${mpiDimensions[1].inSpace($mpiDimensions[1].transformMask).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_unswapped_backward;
${createGuruDFTPlanForVector(vector, 'backward')}@slurp
    @else
_r2r_iodim_ptr = &_r2r_sizes[_r2r_sizes_index];
_r2r_kinds[_r2r_sizes_index++] = ${r2rKindForDimensionAndDirection($mpiDimensions[1], 'forward')};
_r2r_iodim_ptr->n = ${mpiDimensions[1].inSpace(0).localLattice};
_r2r_iodim_ptr->is = ${r2rMultiplier} * _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};
_r2r_iodim_ptr->os = _r2r_iodim_ptr->is;

_space_transform _${vector.id}_partial_unswapped_forward(${mpiDimensions[1].inSpace(0).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_unswapped_forward;
${createGuruR2RPlanForVector(vector)}@slurp
_r2r_kinds[0] = ${r2rKindForDimensionAndDirection($mpiDimensions[1], 'backward')};
_space_transform _${vector.id}_partial_unswapped_backward(${mpiDimensions[1].inSpace($mpiDimensions[1].transformMask).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_unswapped_backward;
${createGuruR2RPlanForVector(vector)}@slurp
    @end if


// 1d. Create local plans for swapped fourier transforms
_forward_sizes_index = _backward_sizes_index = _r2r_sizes_index = 0;
_forward_loop_sizes_index = _backward_loop_sizes_index = _r2r_loop_sizes_index = 0;

//     Loop over the distributed dimension '$mpiDimensions[1].name'
    @if 'dft' in $distributedMPIKinds
_forward_iodim_ptr = &_forward_loop_sizes[_forward_loop_sizes_index++];
_backward_iodim_ptr = &_backward_loop_sizes[_backward_loop_sizes_index++];

_forward_iodim_ptr->n  = ${mpiDimensions[1].inSpace($swappedSpace).localLattice};
_forward_iodim_ptr->is = _${vector.id}_ncomponents * ${mpiDimensions[0].inSpace($swappedSpace).localLattice} * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};

_backward_iodim_ptr->n = _forward_iodim_ptr->n;
_backward_iodim_ptr->os = _forward_iodim_ptr->os = _backward_iodim_ptr->is = _forward_iodim_ptr->is;
    @else
_r2r_iodim_ptr = &_r2r_loop_sizes[_r2r_loop_sizes_index++];
_r2r_iodim_ptr->n = ${mpiDimensions[1].inSpace($swappedSpace).localLattice};
_r2r_iodim_ptr->is = ${r2rMultiplier} * _${vector.id}_ncomponents * ${mpiDimensions[0].inSpace($swappedSpace).localLattice} * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};
_r2r_iodim_ptr->os = _r2r_iodim_ptr->is;
    @end if

//     Forward transform the local MPI dimension '$mpiDimensions[0].name'
    @if 'dft' in $distributedMPIKinds
_forward_iodim_ptr = &_forward_sizes[_forward_sizes_index++];
_backward_iodim_ptr = &_backward_sizes[_backward_sizes_index++];

_forward_iodim_ptr->n  = ${mpiDimensions[0].inSpace($swappedSpace).localLattice};
_forward_iodim_ptr->is = _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};

_backward_iodim_ptr->n = _forward_iodim_ptr->n;
_backward_iodim_ptr->os = _forward_iodim_ptr->os = _backward_iodim_ptr->is = _forward_iodim_ptr->is;

_space_transform _${vector.id}_partial_swapped_forward(${mpiDimensions[0].inSpace(0).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_swapped_forward;
${createGuruDFTPlanForVector(vector, 'forward')}@slurp
_space_transform _${vector.id}_partial_swapped_backward(${mpiDimensions[0].inSpace($mpiDimensions[0].transformMask).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_swapped_backward;
${createGuruDFTPlanForVector(vector, 'backward')}@slurp
    @else
_r2r_iodim_ptr = &_r2r_sizes[_r2r_sizes_index];
_r2r_kinds[_r2r_sizes_index++] = ${r2rKindForDimensionAndDirection($mpiDimensions[0], 'forward')};
_r2r_iodim_ptr->n = ${mpiDimensions[0].inSpace($swappedSpace).localLattice};
_r2r_iodim_ptr->is = ${r2rMultiplier} * _${vector.id}_ncomponents * ${vector.field.pointsInDimensionsWithIndices(range(2, len(vector.field.dimensions)))};
_r2r_iodim_ptr->os = _r2r_iodim_ptr->is;

_space_transform _${vector.id}_partial_swapped_forward(${mpiDimensions[1].inSpace(0).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_swapped_forward;
${createGuruR2RPlanForVector(vector)}@slurp
_r2r_kinds[0] = ${r2rKindForDimensionAndDirection($mpiDimensions[0], 'backward')};
_space_transform _${vector.id}_partial_swapped_backward(${mpiDimensions[1].inSpace($mpiDimensions[1].transformMask).stepSize} * _inverse_sqrt_2pi);
_transform = &_${vector.id}_partial_swapped_backward;
${createGuruR2RPlanForVector(vector)}@slurp
    @end if
  @end if
  @#
  @super(vector)
  @#
@end def


@def setupLocalTransformsForVector($vector)
  @#
  @if not $_driver.hasattr('distributedTransform')
    @return
  @end if
// First check if we can use the full forward or reverse transforms
if (!(_from_space & _${vector.id}_full_mask) && ((_to_space & _${vector.id}_full_mask) == _${vector.id}_full_mask)) {
  // using full forward transform
  _transform->append(_${vector.id}_forward);
  // update from space so that we don't do transforms twice
  _from_space |= _${vector.id}_full_mask;
}
else if (((_from_space & _${vector.id}_full_mask) == _${vector.id}_full_mask) && !(_to_space & _${vector.id}_full_mask)) {
  // using full backward transform
  _transform->append(_${vector.id}_backward);
  _from_space ^= _${vector.id}_full_mask;
}
  @if $vectorNeedsPartialTransforms(vector)
// Now check for partial 2d transforms
    @set $partial2DMask = reduce(operator.__or__, [dim.transformMask for dim in vector.field.dimensions if dim.isDistributed])
    @assert partial2DMask == 6
else if ((_from_space & ${partial2DMask}) != (_to_space & ${partial2DMask})) {
  // There is some transform required in the distributed dimensions
    @# The partial 2D mask should be 6 as they should be the first 2 dimensions excluding the propagation dimension
  unsigned long _2d_oldspace = _from_space & 6;
  unsigned long _2d_newspace = _to_space & 6;
  
  // Consider all transforms between the first two dimensions. There are twelve.
  // Note that 6 has the second lowest and third lowest bits set corresponding to
  // the spaceMask for the first two transverse dimensions.
  // These cases are sorted by the number of operations involved (for no good reason)
  // The names of the dimensions don't necessarily correspond to those in the simulation,
  // but instead to the names in the xpdeint document "doc/FFTW3 Design.pdf"
  // Case 1:  x,  y -> kx, ky
  if (_2d_oldspace == 0 && _2d_newspace == 6)
    _transform->append(_${vector.id}_partial_2d_forward);
  // Case 2: kx, ky ->  x,  y
  else if (_2d_oldspace == 6 && _2d_newspace == 0)
    _transform->append(_${vector.id}_partial_2d_backward);
  // Case 3:  x,  y ->  x, ky
  else if (_2d_oldspace == 0 && _2d_newspace == 4)
    _transform->append(_${vector.id}_partial_unswapped_forward);
  // Case 4:  x, ky ->  x,  y
  else if (_2d_oldspace == 4 && _2d_newspace == 0)
    _transform->append(_${vector.id}_partial_unswapped_backward);
  // Case 5:  x, ky -> kx, ky
  else if (_2d_oldspace == 4 && _2d_newspace == 6) {
    _transform->append(_${vector.id}_partial_transpose);
    _transform->append(_${vector.id}_partial_swapped_forward);
  }
  // Case 6: kx, ky ->  x, ky
  else if (_2d_oldspace == 6 && _2d_newspace == 4) {
    _transform->append(_${vector.id}_partial_swapped_backward);
    _transform->append(_${vector.id}_partial_untranspose);
  }
  // Case 7: kx,  y -> kx, ky
  else if (_2d_oldspace == 2 && _2d_newspace == 6) {
    _transform->append(_${vector.id}_partial_unswapped_forward);
    _transform->append(_${vector.id}_partial_transpose);
  }
  // Case 8: kx, ky -> kx,  y
  else if (_2d_oldspace == 6 && _2d_newspace == 2) {
    _transform->append(_${vector.id}_partial_untranspose);
    _transform->append(_${vector.id}_partial_unswapped_backward);
  }
  // Case 9:  x,  y -> kx,  y
  else if (_2d_oldspace == 0 && _2d_newspace == 2) {
    _transform->append(_${vector.id}_partial_transpose);
    _transform->append(_${vector.id}_partial_swapped_forward);
    _transform->append(_${vector.id}_partial_untranspose);
  }
  // Case 10: kx,  y ->  x,  y
  else if (_2d_oldspace == 2 && _2d_newspace == 0) {
    _transform->append(_${vector.id}_partial_transpose);
    _transform->append(_${vector.id}_partial_swapped_backward);
    _transform->append(_${vector.id}_partial_untranspose);
  }
  // Case 11:  x, ky -> kx,  y
  else if (_2d_oldspace == 4 && _2d_newspace == 2) {
    _transform->append(_${vector.id}_partial_unswapped_backward);
    _transform->append(_${vector.id}_partial_transpose);
    _transform->append(_${vector.id}_partial_swapped_forward);
    _transform->append(_${vector.id}_partial_untranspose);
  }
  // Case 12: kx,  y ->  x, ky
  else if (_2d_oldspace == 2 && _2d_newspace == 4) {
    _transform->append(_${vector.id}_partial_transpose);
    _transform->append(_${vector.id}_partial_swapped_backward);
    _transform->append(_${vector.id}_partial_untranspose);
    _transform->append(_${vector.id}_partial_unswapped_forward);
  }
  else
    _LOG(_ERROR_LOG_LEVEL, "Missing combination of spaces? _2d_oldspace: %lu, _2d_newspace: %lu\n", _2d_oldspace, _2d_newspace);
}
  @end if
  @#
@end def

@def addOtherTransformsForVector($vector)
  @if not $vector.field.isDistributed
    @return
  @end if
  @#
@end def


@def executePlanForVector($vector)
  @#
  @super(vector)
  @#
case _MPI_DFT_TRANSFORM:
case _MPI_R2R_TRANSFORM:
case _MPI_TRANSPOSE:
  ${fftwPrefix}_execute_r2r(_it->first, reinterpret_cast<real*>(_active_${vector.id}),
                               reinterpret_cast<real*>(_active_${vector.id}));
  break;
  @#
@end def
