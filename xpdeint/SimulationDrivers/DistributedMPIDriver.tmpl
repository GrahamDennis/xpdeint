@*
DistributedMPIDriver.tmpl

Created by Graham Dennis on 2008-03-28.
Copyright (c) 2008 __MyCompanyName__. All rights reserved.
*@
@extends xpdeint.SimulationDrivers._DistributedMPIDriver

@def description: Distributed MPI Simulation Driver

@def mainRoutine
  @#
int main(int argc, char **argv)
{
  MPI_Init(&argc, &argv);
  MPI_Comm_size(MPI_COMM_WORLD, &_size);
  MPI_Comm_rank(MPI_COMM_WORLD, &_rank);
  
  ${distributedTransform.setLocalLatticeAndOffsetVariables, autoIndent=True}@slurp
  
  ${mainRoutineInnerContent, autoIndent=True}@slurp
  
  MPI_Finalize();
  
  return 0;
}
  @#
@end def

@def loopOverFieldInSpaceWithVectorsAndInnerContentEnd($dict)
  @#
  @set $vectorOverrides = dict['vectorOverrides']
  @set $indexOverrides = dict['indexOverrides']
  @set $field = dict['field']
  @set $space = dict['space']
  @set $mpiDimension = $mpiDimensionForSpace($space)
  @#
  @for $vector in $vectorOverrides
    @set $dimensionsIntegratedOver = set([d.inSpace(space).name for d in field.dimensions if not vector.field.hasDimension(d)])
    @if not mpiDimension.inSpace(space).name in dimensionsIntegratedOver
      @# If we aren't integrating over an MPI dimension, then everything is as usual.
      @continue
    @end if
    @# We did integrate over the MPI dimension, so we need to run MPI_Allreduce to combine the results.
    @set $arrayName = c'_active_${vector.id}'
    @set $size = $sizeOfVectorInSpaceInReals(vector, space)
    @#
    @# If we have any dimension overrides, then we don't want to add up the entire field
    @for $dimName in indexOverrides.iterkeys()
      @if vector.field.hasDimensionName(dimName) and vector.field in indexOverrides[dimName]
        @set $indexOverride = indexOverrides[dimName][vector.field]
        @set $vectorDim = vector.field.dimensionWithName(dimName)
        @set $arrayName = arrayName + c' + ${indexOverride} * ${vector.field.localPointsInDimensionsAfterDimensionInSpace(vectorDim, space)} * _${vector.id}_ncomponents'
        @set $size = size + c' / _${vector.field.name}_lattice_${dimName}'
      @end if
    @end for
MPI_Allreduce(MPI_IN_PLACE, $arrayName, $size,
              MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
  @end for
  @#
@end def

@def findMax($dict)
  @#
  @set $variable = dict['variable']
  @set $count = dict['count']
MPI_Allreduce(MPI_IN_PLACE, $variable, $count, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);
  @#
@end def


@def binaryWriteOutBegin($dict)
  @#
// Only write to file if we are rank 0, as we cannot assume
// that the nodes have equal access to the filesystem
if (_rank == 0) {
  @set $dict['extraIndent'] += 2
  @#
@end def

@def binaryWriteOutEnd($dict)
  @#
  @set $field = dict['field']
  @set $space = dict['space']
  @set $dependentVariables = dict['dependentVariables']
  @#
  @set $dict['extraIndent'] -= 2
  @#
}
  @if not all([field.hasDimensionName(dimName) for dimName in $distributedDimensionNames])
    @# If we don't have all the MPI dimensions, then the data will be local.
    @stop
  @end if
else {
  // We are some other rank that isn't 0, so we need to send our data to rank 0.
  ptrdiff_t _sending_var;
  
  @for $shadowVariable in $shadowedVariablesForField(field)
  _sending_var = $shadowVariable;
  MPI_Ssend(&_sending_var, sizeof(ptrdiff_t), MPI_BYTE, 0, 0, MPI_COMM_WORLD);
  @end for
  _sending_var = ${sizeOfFieldInSpace(field, space)};
  MPI_Ssend(&_sending_var, sizeof(ptrdiff_t), MPI_BYTE, 0, 0, MPI_COMM_WORLD);
  
  @# Note that a variable corresponds to an array with given component names
  @for $variable in $dependentVariables
  MPI_Ssend(${variable.arrayName}, ${sizeOfVectorInSpaceInReals($variable.vector, space)}, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
  @end for
}

MPI_Barrier(MPI_COMM_WORLD);
  @#
@end def

@def binaryWriteOutWriteDataBegin($dict)
  @#
  @set $field = dict['field']
  @set $space = dict['space']
  @set $dependentVariables = dict['dependentVariables']
  @#
  @if not all([field.hasDimensionName(dimName) for dimName in $distributedDimensionNames])
    @# If we don't have all the MPI dimensions, then the data will be local.
    @return
  @end if
  @#
  @for $shadowVariable in $shadowedVariablesForField(field)
ptrdiff_t _my${shadowVariable} = ${shadowVariable};
  @end for
  @#

@for $variable in $dependentVariables
${variable.vector.type}* _local${variable.arrayName};
${variable.vector.type}* _backup${variable.arrayName} = ${variable.arrayName};
@end for

for (long _dataForRank = 0; _dataForRank < _size; _dataForRank++) {
  @for $shadowVariable in $shadowedVariablesForField(field)
  ptrdiff_t ${shadowVariable};
  @end for
  ptrdiff_t _local_field_size;
  
  if (_dataForRank == 0) {
  @for $shadowVariable in $shadowedVariablesForField(field)
    ${shadowVariable} = _my${shadowVariable};
  @end for
  
  } else {
    MPI_Status status;
  @for $shadowVariable in $shadowedVariablesForField(field)
    MPI_Recv(&${shadowVariable}, sizeof(ptrdiff_t), MPI_BYTE, _dataForRank, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
  @end for
    MPI_Recv(&_local_field_size, sizeof(ptrdiff_t), MPI_BYTE, _dataForRank, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
    
    
    // Now allocate the space needed locally, and receive the entire buffer
  @for $variable in $dependentVariables
    @set $sizePrefix = ''
    @if $variable.vector.type == 'complex'
      @set $sizePrefix = '2 * '
    @end if
    _local${variable.arrayName} = (${variable.vector.type}*) xmds_malloc(sizeof(${variable.vector.type}) * _local_field_size * _${variable.vector.id}_ncomponents);
    MPI_Recv(_local${variable.arrayName}, ${sizePrefix}_local_field_size * _${variable.vector.id}_ncomponents,
             MPI_DOUBLE, _dataForRank, MPI_ANY_TAG, MPI_COMM_WORLD, &status);
    ${variable.arrayName} = _local${variable.arrayName};
    
  @end for
  }
  @set $dict['extraIndent'] += 2
  @#
@end def

@def binaryWriteOutWriteDataEnd($dict)
  @#
  @set $field = dict['field']
  @set $dict['extraIndent'] -= 2
  @set $dependentVariables = dict['dependentVariables']
  @#
  @if not all([field.hasDimensionName(dimName) for dimName in $distributedDimensionNames])
    @# If we don't have all the MPI dimensions, then the data will be local.
    @return
  @end if
  @#
  
  if (_dataForRank != 0) {
  @for $variable in $dependentVariables
    xmds_free(_local${variable.arrayName});
  @end for
  }
} // End looping over ranks
  @for $vector in $dependentVariables
${variable.arrayName} = _backup${variable.arrayName};
  @end for

  @#
@end def


@def generateNoisesBegin($dict)
  @#
  @set $field = dict['field']
  @set $space = dict['space']
  @set $noises = dict['noises']
  @#
  @if $field.hasDimension($mpiDimensionForSpace(space))
    @# If the field has the MPI dimension, then the noise
    @# will need to vary along the MPI dimension, so all is
    @# OK.
    @return
  @end if
  @#
  @# This means that the noise field doesn't contain the MPI dimension.
  @# As a result, the noise vector should be identical
if (_rank == 0) {
  // This noise doesn't contain the MPI dimension ('${mpiDimensionForSpace($space).name}'), so we should
  // make sure the noise is the same on all ranks
  @set $dict['extraIndent'] += 2
  @#
@end def

@def generateNoisesEnd($dict)
  @#
  @set $field = dict['field']
  @set $space = dict['space']
  @set $noises = dict['noises']
  @#
  @if $field.hasDimension($mpiDimensionForSpace(space))
    @# If the field has the MPI dimension, then the noise
    @# will need to vary along the MPI dimension, so all is
    @# OK.
    @return
  @end if
  @#
}
// Broadcast the noises to other nodes
  @for $noise in $noises
    @set $noiseVector = noise.noiseVectorForField(field)
MPI_Bcast(_active_${noiseVector.id}, ${sizeOfVectorInSpaceInReals(noiseVector, space)}, MPI_DOUBLE, 0, MPI_COMM_WORLD);
  @end for

  @set $dict['extraIndent'] -= 2
  @#
@end def


